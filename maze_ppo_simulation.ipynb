{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze Robot PPO Simulation\n",
    "\n",
    "This notebook runs the Proximal Policy Optimization (PPO) simulation for maze robot navigation in a headless environment and captures video for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pygame\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "from IPython.display import HTML, display\n",
    "from base64 import b64encode\n",
    "from IPython.display import Video\n",
    "\n",
    "# Import project modules\n",
    "from Robot_config import Robot\n",
    "from Environment import Environment\n",
    "from ppo_agent import PPO\n",
    "from reward_model import RewardModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup headless pygame\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "pygame.init()\n",
    "\n",
    "# Configure video recording\n",
    "def setup_video_writer(filename, fps=30, size=(900, 900)):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'XVID'\n",
    "    return cv2.VideoWriter(filename, fourcc, fps, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert pygame surface to numpy array for OpenCV\n",
    "def surface_to_array(surface):\n",
    "    return np.transpose(np.array(pygame.surfarray.pixels3d(surface)), (1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display video in the notebook\n",
    "def show_video(file_path, width=640):\n",
    "    return Video(file_path, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ppo_simulation(max_episodes=50, video_filename=\"ppo_simulation.mp4\", record_interval=2):\n",
    "    # Record only certain episodes to keep video size manageable\n",
    "    record_episodes = set([0, max_episodes//4, max_episodes//2, max_episodes-1])\n",
    "    \n",
    "    # Load map\n",
    "    map = pygame.image.load(\"./maze.jpg\")\n",
    "    map_copy = map.copy()\n",
    "    \n",
    "    # Start position & size maze\n",
    "    start_position = (450, 100)  # origin 450, 100\n",
    "    end_position = (450, 900, -np.pi/2)\n",
    "    size = (900, 900)\n",
    "    \n",
    "    environment = Environment(size, \"./maze.jpg\")\n",
    "    \n",
    "    # Robot setup\n",
    "    robot = Robot(start_position, \"./car.png\", 2, 2)\n",
    "    \n",
    "    # Setup PPO agent\n",
    "    state_dim = 11  # 8 sensors + x, y, theta\n",
    "    action_dim = 2  # vx, vtheta\n",
    "    ppo_agent = PPO(state_dim=state_dim, action_dim=action_dim, hidden_dim=128,\n",
    "                    lr=3e-4, gamma=0.99, clip_ratio=0.2, batch_size=32, epochs=5)\n",
    "    \n",
    "    # Setup reward model\n",
    "    reward_model = RewardModel(target_position=end_position[:2], target_orientation=end_position[2])\n",
    "    \n",
    "    # Video writer\n",
    "    video_writer = setup_video_writer(video_filename)\n",
    "    \n",
    "    # Metrics to track\n",
    "    episode_rewards = []\n",
    "    distances_to_goal = []\n",
    "    best_reward = -float('inf')\n",
    "    best_distance = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    episode = 0\n",
    "    dt = 0.05  # Fixed dt for simulation stability\n",
    "    \n",
    "    try:\n",
    "        while episode < max_episodes:\n",
    "            print(f\"Processing episode {episode+1}/{max_episodes}\")\n",
    "            \n",
    "            # Record this episode?\n",
    "            record_this_episode = episode in record_episodes\n",
    "            frame_counter = 0\n",
    "            \n",
    "            # Reset environment\n",
    "            robot.reset(start_position)\n",
    "            \n",
    "            # Reset reward model\n",
    "            reward_model.reset((robot.x, robot.y))\n",
    "            \n",
    "            # Initial state\n",
    "            robot.update_sensor_data(map_copy, (0, 0, 0))\n",
    "            state = robot.get_state()\n",
    "            \n",
    "            episode_reward = 0\n",
    "            step = 0\n",
    "            done = False\n",
    "            max_steps_per_episode = 500\n",
    "            \n",
    "            while not done and step < max_steps_per_episode:\n",
    "                # Clear display surface for this frame\n",
    "                if record_this_episode and frame_counter % record_interval == 0:\n",
    "                    environment.map.blit(map, (0, 0))\n",
    "                \n",
    "                # Get action from policy\n",
    "                action, log_prob = ppo_agent.select_action(state)\n",
    "                \n",
    "                # Scale action\n",
    "                robot.vx = action[0] * 150  # Scale to reasonable range\n",
    "                robot.vtheta = action[1] * 1.0  # Scale to reasonable range\n",
    "                \n",
    "                # Execute action\n",
    "                robot.move(dt)\n",
    "                robot.time += dt\n",
    "                robot.update_sensor_data(map_copy, (0, 0, 0))\n",
    "                robot.check_crash(map_copy, (0, 0, 0))\n",
    "                \n",
    "                # Get new state\n",
    "                next_state = robot.get_state()\n",
    "                \n",
    "                # Calculate reward and check if done\n",
    "                robot_state = {\n",
    "                    'position': (robot.x, robot.y),\n",
    "                    'orientation': robot.theta,\n",
    "                    'velocity': (robot.x_dot, robot.y_dot),\n",
    "                    'sensor_data': robot.sensor_data,\n",
    "                    'crash': robot.crash,\n",
    "                    'time': robot.time\n",
    "                }\n",
    "                reward, done = reward_model.calculate_reward(robot_state)\n",
    "                \n",
    "                # Store transition in PPO memory\n",
    "                ppo_agent.store_transition(state, action, reward, next_state, done, log_prob)\n",
    "                \n",
    "                # Update state and accumulate reward\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step += 1\n",
    "                \n",
    "                # Visualization - only record certain frames to keep video size manageable\n",
    "                if record_this_episode and frame_counter % record_interval == 0:\n",
    "                    robot.draw(environment.map)\n",
    "                    environment.robot_frames([robot.x, robot.y], robot.theta)\n",
    "                    environment.robot_sensor([robot.x, robot.y], robot.points)\n",
    "                    environment.trail((robot.x, robot.y))\n",
    "                    \n",
    "                    # Add episode info to frame\n",
    "                    font = pygame.font.Font('freesansbold.ttf', 24)\n",
    "                    text = font.render(f\"Episode: {episode+1}/{max_episodes}, Reward: {episode_reward:.2f}\", True, (255, 255, 255), (0, 0, 0))\n",
    "                    environment.map.blit(text, (10, 10))\n",
    "                    \n",
    "                    # Record frame\n",
    "                    frame = surface_to_array(environment.map)\n",
    "                    # Convert to BGR for OpenCV\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                    video_writer.write(frame)\n",
    "                \n",
    "                frame_counter += 1\n",
    "            \n",
    "            # Update policy every 5 episodes\n",
    "            if (episode + 1) % 5 == 0:\n",
    "                ppo_agent.update()\n",
    "            \n",
    "            # Record stats\n",
    "            episode_rewards.append(episode_reward)\n",
    "            distance_to_goal = math.sqrt((robot.x - end_position[0])**2 + (robot.y - end_position[1])**2)\n",
    "            distances_to_goal.append(distance_to_goal)\n",
    "            \n",
    "            # Save best model\n",
    "            if episode_reward > best_reward:\n",
    "                best_reward = episode_reward\n",
    "                torch.save(ppo_agent.actor_critic.state_dict(), \"ppo_best_model.pt\")\n",
    "            \n",
    "            if distance_to_goal < best_distance:\n",
    "                best_distance = distance_to_goal\n",
    "            \n",
    "            print(f\"Episode: {episode+1}, Reward: {episode_reward:.2f}, Distance: {distance_to_goal:.2f}, Best Distance: {best_distance:.2f}\")\n",
    "            episode += 1\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Simulation interrupted by user\")\n",
    "    \n",
    "    finally:\n",
    "        # Save final model\n",
    "        torch.save(ppo_agent.actor_critic.state_dict(), \"ppo_final_model.pt\")\n",
    "        # Release video writer\n",
    "        video_writer.release()\n",
    "    \n",
    "    return episode_rewards, distances_to_goal, video_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import math for distance calculation\n",
    "import math\n",
    "\n",
    "# Run simulation\n",
    "rewards, distances, video_file = run_ppo_simulation(max_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot rewards over episodes\n",
    "ax1.plot(range(1, len(rewards) + 1), rewards, 'b-')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Reward')\n",
    "ax1.set_title('Reward Evolution During Training')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Calculate moving average for reward trend line\n",
    "window_size = 5\n",
    "if len(rewards) > window_size:\n",
    "    moving_avg = [np.mean(rewards[i:i+window_size]) for i in range(len(rewards)-window_size+1)]\n",
    "    ax1.plot(range(window_size, len(rewards) + 1), moving_avg, 'r-', label=f'{window_size}-Episode Moving Average')\n",
    "    ax1.legend()\n",
    "\n",
    "# Plot distance to goal over episodes\n",
    "ax2.plot(range(1, len(distances) + 1), distances, 'g-')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Distance to Goal (Pixels)')\n",
    "ax2.set_title('Distance to Goal at End of Episode')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ppo_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the video\n",
    "show_video(video_file, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Trained Model\n",
    "\n",
    "After training, we can evaluate the best model to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ppo_model(model_path=\"ppo_best_model.pt\", episodes=5, video_filename=\"ppo_evaluation.mp4\"):\n",
    "    # Load map\n",
    "    map = pygame.image.load(\"./maze.jpg\")\n",
    "    map_copy = map.copy()\n",
    "    \n",
    "    # Start position & size maze\n",
    "    start_position = (450, 100)\n",
    "    end_position = (450, 900, -np.pi/2)\n",
    "    size = (900, 900)\n",
    "    \n",
    "    # Environment setup\n",
    "    environment = Environment(size, \"./maze.jpg\")\n",
    "    \n",
    "    # Robot setup\n",
    "    robot = Robot(start_position, \"./car.png\", 2, 2)\n",
    "    \n",
    "    # Setup PPO agent for evaluation\n",
    "    state_dim = 11  # 8 sensors + x, y, theta\n",
    "    action_dim = 2  # vx, vtheta\n",
    "    \n",
    "    # Create model and load weights\n",
    "    actor_critic = ActorCritic(state_dim, action_dim)\n",
    "    actor_critic.load_state_dict(torch.load(model_path))\n",
    "    actor_critic.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Create PPO agent for evaluation only\n",
    "    ppo_agent = PPO(state_dim, action_dim)\n",
    "    ppo_agent.actor_critic = actor_critic\n",
    "    \n",
    "    # Video writer\n",
    "    video_writer = setup_video_writer(video_filename)\n",
    "    \n",
    "    # Stats collection\n",
    "    success_count = 0\n",
    "    distances = []\n",
    "    times = []\n",
    "    \n",
    "    dt = 0.05  # Fixed dt for simulation stability\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Reset robot\n",
    "        robot.reset(start_position)\n",
    "        \n",
    "        # Initial state\n",
    "        robot.update_sensor_data(map_copy, (0, 0, 0))\n",
    "        state = robot.get_state()\n",
    "        \n",
    "        running = True\n",
    "        frame_counter = 0\n",
    "        \n",
    "        while running:\n",
    "            # Clear display surface\n",
    "            environment.map.blit(map, (0, 0))\n",
    "            \n",
    "            # Get action from policy (deterministic for evaluation)\n",
    "            action = ppo_agent.select_action(state, deterministic=True)\n",
    "            \n",
    "            # Scale and execute action\n",
    "            robot.vx = action[0] * 150\n",
    "            robot.vtheta = action[1] * 1.0\n",
    "            \n",
    "            # Execute action\n",
    "            robot.move(dt)\n",
    "            robot.time += dt\n",
    "            robot.update_sensor_data(map_copy, (0, 0, 0))\n",
    "            robot.check_crash(map_copy, (0, 0, 0))\n",
    "            \n",
    "            # Get new state\n",
    "            state = robot.get_state()\n",
    "            \n",
    "            # Check if episode is done\n",
    "            distance_to_goal = math.sqrt((robot.x - end_position[0])**2 + (robot.y - end_position[1])**2)\n",
    "            orientation_diff = abs(((robot.theta - end_position[2] + np.pi) % (2 * np.pi)) - np.pi)\n",
    "            \n",
    "            # Check success conditions\n",
    "            success = distance_to_goal < 30 and orientation_diff < 0.5\n",
    "            done = robot.crash or robot.time > 30 or success\n",
    "            \n",
    "            if done:\n",
    "                distances.append(distance_to_goal)\n",
    "                times.append(robot.time)\n",
    "                if success:\n",
    "                    success_count += 1\n",
    "                running = False\n",
    "            \n",
    "            # Visualization\n",
    "            robot.draw(environment.map)\n",
    "            environment.robot_frames([robot.x, robot.y], robot.theta)\n",
    "            environment.robot_sensor([robot.x, robot.y], robot.points)\n",
    "            environment.trail((robot.x, robot.y))\n",
    "            \n",
    "            # Show additional info\n",
    "            font = pygame.font.Font('freesansbold.ttf', 20)\n",
    "            info_text = f\"Evaluation Episode: {episode+1}/{episodes}, Time: {robot.time:.1f}s, Distance: {distance_to_goal:.1f}\"\n",
    "            text = font.render(info_text, True, (255, 255, 255), (0, 0, 0))\n",
    "            environment.map.blit(text, (10, 10))\n",
    "            \n",
    "            # Record frame\n",
    "            if frame_counter % 2 == 0:  # Record every other frame to save space\n",
    "                frame = surface_to_array(environment.map)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                video_writer.write(frame)\n",
    "            \n",
    "            frame_counter += 1\n",
    "            \n",
    "            # Limit evaluation to reasonable number of frames\n",
    "            if frame_counter > 1000:\n",
    "                running = False\n",
    "                distances.append(distance_to_goal)\n",
    "                times.append(robot.time)\n",
    "        \n",
    "        print(f\"Episode {episode+1}: {'Success' if success else 'Failed'}, Distance: {distance_to_goal:.2f}, Time: {robot.time:.2f}s\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    success_rate = success_count / episodes * 100\n",
    "    avg_distance = sum(distances) / len(distances)\n",
    "    avg_time = sum(times) / len(times)\n",
    "    \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Success Rate: {success_rate:.1f}%\")\n",
    "    print(f\"Average Distance to Goal: {avg_distance:.2f}\")\n",
    "    print(f\"Average Episode Time: {avg_time:.2f}s\")\n",
    "    \n",
    "    # Release video writer\n",
    "    video_writer.release()\n",
    "    \n",
    "    return video_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ActorCritic for evaluation\n",
    "from ppo_agent import ActorCritic\n",
    "\n",
    "# Evaluate the trained model\n",
    "eval_video = evaluate_ppo_model(episodes=3)\n",
    "show_video(eval_video, width=720)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-mmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

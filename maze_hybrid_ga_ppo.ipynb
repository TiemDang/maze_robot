{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze Robot Hybrid GA-PPO Approach\n",
    "\n",
    "This notebook implements a hybrid approach that combines Genetic Algorithm (GA) with Proximal Policy Optimization (PPO). \n",
    "The key idea is to first use GA to evolve neural network architectures, then apply PPO to fine-tune the best policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pygame\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from IPython.display import Video\n",
    "\n",
    "# Import project modules\n",
    "from Robot_config import Robot\n",
    "from Environment import Environment\n",
    "from class_GA import Genetic_Algo\n",
    "from ppo_agent import PPO, ActorCritic\n",
    "from reward_model import RewardModel\n",
    "from ClassNeuralNetwork import NeuralNet\n",
    "from save_metrics import save_ga_metrics, save_ppo_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup headless pygame\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "pygame.init()\n",
    "\n",
    "# Configure video recording\n",
    "def setup_video_writer(filename, fps=30, size=(900, 900)):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    return cv2.VideoWriter(filename, fourcc, fps, size)\n",
    "\n",
    "# Helper function to convert pygame surface to numpy array for OpenCV\n",
    "def surface_to_array(surface):\n",
    "    return np.transpose(np.array(pygame.surfarray.pixels3d(surface)), (1, 0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid GA-PPO Approach\n",
    "\n",
    "Our hybrid approach works as follows:\n",
    "\n",
    "1. Use GA to evolve a population of neural networks (similar to the original GA solution)\n",
    "2. Take the best N individuals from the GA population\n",
    "3. Convert each individual to an Actor-Critic architecture for PPO\n",
    "4. Apply PPO to fine-tune each of these networks\n",
    "5. Select the best performing network after PPO fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a GA individual (neural network weights) to a PPO actor-critic model\n",
    "def convert_ga_to_ppo_model(decoded_individual, state_dim=11, action_dim=2, hidden_dim=128):\n",
    "    \"\"\"\n",
    "    Convert GA individual to PPO actor-critic model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    decoded_individual : array\n",
    "        The decoded GA individual containing neural network weights\n",
    "    \"\"\"\n",
    "    # Create a new PPO actor-critic model\n",
    "    actor_critic = ActorCritic(state_dim, action_dim, hidden_dim)\n",
    "    \n",
    "    # Since GA uses a simple neural network and PPO uses a more complex architecture,\n",
    "    # we'll initialize the feature extractor using weights from the GA individual\n",
    "    w = np.reshape(decoded_individual[:110], (11, 10))  # First 110 elements are input->hidden weights\n",
    "    v = np.reshape(decoded_individual[110:], (10, 2))   # Remaining elements are hidden->output weights\n",
    "    \n",
    "    # Set the first layer of feature extractor with GA weights\n",
    "    with torch.no_grad():\n",
    "        # Convert numpy arrays to PyTorch tensors\n",
    "        w_tensor = torch.FloatTensor(w.T)  # Transpose for PyTorch's expected dimensions\n",
    "        \n",
    "        # Copy weights to PPO model's feature extractor first layer (with size adjustment)\n",
    "        actor_critic.feature_extractor[0].weight.data[:10, :11] = w_tensor\n",
    "        \n",
    "        # Set output layer of actor with GA weights\n",
    "        v_tensor = torch.FloatTensor(v.T)  # Transpose for PyTorch's expected dimensions\n",
    "        actor_critic.actor_mean.weight.data[:2, :10] = v_tensor\n",
    "    \n",
    "    return actor_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hybrid_ga_ppo(ga_generations=15, ppo_episodes=20, population_size=30, \n",
    "                     top_n_individuals=3, video_filename=\"hybrid_simulation.mp4\"):\n",
    "    print(\"Starting Hybrid GA-PPO Simulation\")\n",
    "    \n",
    "    # Setup environment\n",
    "    map = pygame.image.load(\"./maze.jpg\")\n",
    "    map_copy = map.copy()\n",
    "    start_position = (450, 100)\n",
    "    end_position = (450, 900, -np.pi/2)\n",
    "    size = (900, 900)\n",
    "    environment = Environment(size, \"./maze.jpg\")\n",
    "    \n",
    "    # Video writer\n",
    "    video_writer = setup_video_writer(video_filename)\n",
    "    \n",
    "    # Step 1: Run GA to evolve initial population\n",
    "    print(\"Phase 1: Running Genetic Algorithm...\")\n",
    "    \n",
    "    # Setup robots\n",
    "    Robots = [Robot(start_position, \"./car.png\", 2, 2) for _ in range(population_size)]\n",
    "    \n",
    "    # GA initialization\n",
    "    GA_init = Genetic_Algo([-5, 5], (-5, 5), population_size, 130, 12)\n",
    "    population = GA_init.create_population()\n",
    "    \n",
    "    # Metrics to track\n",
    "    min_fitness_history = []\n",
    "    mean_fitness_history = []\n",
    "    best_distance_history = []\n",
    "    best_individuals = []\n",
    "    \n",
    "    # GA loop\n",
    "    for generation in range(ga_generations):\n",
    "        print(f\"GA Generation {generation+1}/{ga_generations}\")\n",
    "        \n",
    "        # Reset robots\n",
    "        for robot in Robots:\n",
    "            robot.reset(start_position)\n",
    "            robot.update_sensor_data(map_copy, (0, 0, 0))\n",
    "        \n",
    "        # Decode population\n",
    "        decode = GA_init.decode_gen(population)\n",
    "        \n",
    "        # Evaluate population\n",
    "        Robot_available = population_size\n",
    "        dt = 0.05  # Fixed dt for simulation stability\n",
    "        \n",
    "        while Robot_available > 0:\n",
    "            environment.map.blit(map, (0, 0))\n",
    "            \n",
    "            for idx, robot in enumerate(Robots):\n",
    "                if not robot.crash:\n",
    "                    # Neural network control from GA\n",
    "                    decode_individual = decode[idx]\n",
    "                    w = np.reshape(decode_individual[:110], (11, 10))\n",
    "                    v = np.reshape(decode_individual[110:], (10, 2))\n",
    "                    \n",
    "                    x = np.array(robot.sensor_data + [float(robot.x), float(robot.y), float(robot.theta)]).reshape(11, 1)\n",
    "                    \n",
    "                    # Get control actions from neural network\n",
    "                    VVV = NeuralNet(x, w, v).FeedForward()\n",
    "                    robot.vx = (VVV[0][0]) * 2.5\n",
    "                    robot.vtheta = (VVV[1][0]) * 0.03\n",
    "                    \n",
    "                    # Move robot\n",
    "                    robot.move(dt)\n",
    "                    robot.time += dt\n",
    "                    robot.check_crash(map_copy, (0, 0, 0))\n",
    "                    \n",
    "                    # Record the end state if crashed\n",
    "                    if robot.crash or robot.time > 30:\n",
    "                        Robot_available -= 1\n",
    "                    \n",
    "                    # Update sensors\n",
    "                    robot.update_sensor_data(map_copy, (0, 0, 0))\n",
    "            \n",
    "            # Record video only for certain frames and generations\n",
    "            if generation in [0, ga_generations//2, ga_generations-1]:\n",
    "                # Visualize\n",
    "                environment.map.blit(map, (0, 0))\n",
    "                for robot in Robots:\n",
    "                    if not robot.crash:\n",
    "                        robot.draw(environment.map)\n",
    "                        environment.robot_frames([robot.x, robot.y], robot.theta)\n",
    "                \n",
    "                # Add generation info\n",
    "                font = pygame.font.Font('freesansbold.ttf', 24)\n",
    "                text = font.render(f\"GA Generation: {generation+1}/{ga_generations}\", True, (255, 255, 255), (0, 0, 0))\n",
    "                environment.map.blit(text, (10, 10))\n",
    "                \n",
    "                # Record frame\n",
    "                frame = surface_to_array(environment.map)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                video_writer.write(frame)\n",
    "        \n",
    "        # Calculate fitness (distance to goal)\n",
    "        fitness = []\n",
    "        for robot in Robots:\n",
    "            dist_to_goal = math.sqrt((robot.x - end_position[0])**2 + (robot.y - end_position[1])**2)\n",
    "            fitness.append(dist_to_goal)\n",
    "        \n",
    "        # Record metrics\n",
    "        min_fitness = np.min(fitness)\n",
    "        mean_fitness = np.mean(fitness)\n",
    "        min_fitness_history.append(min_fitness)\n",
    "        mean_fitness_history.append(mean_fitness)\n",
    "        best_distance_history.append(min_fitness)\n",
    "        \n",
    "        # Sort individuals by fitness (distance)\n",
    "        sorted_indices = np.argsort(fitness)\n",
    "        \n",
    "        # Store the top N individuals\n",
    "        best_individuals = [decode[i] for i in sorted_indices[:top_n_individuals]]\n",
    "        \n",
    "        # GA operations for next generation\n",
    "        select_population = GA_init.selection(fitness, population)\n",
    "        cross_population = GA_init.crossover(select_population)\n",
    "        population = GA_init.mutation(cross_population, 0.2)\n",
    "        \n",
    "        print(f\"Generation {generation+1} completed. Best distance: {min_fitness:.2f}, Mean: {mean_fitness:.2f}\")\n",
    "    \n",
    "    # Save GA metrics\n",
    "    save_ga_metrics(min_fitness_history, mean_fitness_history, best_distance_history, 'hybrid_ga_metrics.npz')\n",
    "    \n",
    "    # Step 2: Convert best GA individuals to PPO models and fine-tune\n",
    "    print(\"\\nPhase 2: PPO Fine-tuning of Best Individuals...\")\n",
    "    \n",
    "    # Setup reward model for PPO\n",
    "    reward_model = RewardModel(target_position=end_position[:2], target_orientation=end_position[2])\n",
    "    \n",
    "    best_ppo_rewards = []\n",
    "    best_ppo_distances = []\n",
    "    best_model = None\n",
    "    best_reward = -float('inf')\n",
    "    \n",
    "    # For each of the top N individuals\n",
    "    for individual_idx, individual in enumerate(best_individuals):\n",
    "        print(f\"\\nFine-tuning Individual {individual_idx+1}/{len(best_individuals)}\")\n",
    "        \n",
    "        # Convert GA individual to PPO model\n",
    "        actor_critic = convert_ga_to_ppo_model(individual)\n",
    "        \n",
    "        # Create PPO agent with this model\n",
    "        ppo_agent = PPO(state_dim=11, action_dim=2, hidden_dim=128)\n",
    "        ppo_agent.actor_critic = actor_critic\n",
    "        \n",
    "        # Track metrics for this individual\n",
    "        individual_rewards = []\n",
    "        individual_distances = []\n",
    "        \n",
    "        # PPO fine-tuning loop\n",
    "        for episode in range(ppo_episodes):\n",
    "            # Reset robot\n",
    "            robot = Robot(start_position, \"./car.png\", 2, 2)\n",
    "            reward_model.reset((robot.x, robot.y))\n",
    "            \n",
    "            # Initial state\n",
    "            robot.update_sensor_data(map_copy, (0, 0, 0))\n",
    "            state = robot.get_state()\n",
    "            \n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            step = 0\n",
    "            max_steps = 500\n",
    "            \n",
    "            while not done and step < max_steps:\n",
    "                # Get action from policy\n",
    "                action, log_prob = ppo_agent.select_action(state)\n",
    "                \n",
    "                # Execute action using robot's kinematic model\n",
    "                robot.vx = action[0] * 2.5  # Scale linear velocity\n",
    "                robot.vtheta = action[1] * 0.03  # Scale angular velocity\n",
    "                robot.move(dt)\n",
    "                robot.time += dt\n",
    "                robot.update_sensor_data(map_copy, (0, 0, 0))\n",
    "                robot.check_crash(map_copy, (0, 0, 0))\n",
    "                \n",
    "                # Get new state\n",
    "                next_state = robot.get_state()\n",
    "                \n",
    "                # Calculate reward\n",
    "                robot_state = {\n",
    "                    'position': (robot.x, robot.y),\n",
    "                    'orientation': robot.theta,\n",
    "                    'velocity': (robot.x_dot, robot.y_dot),\n",
    "                    'sensor_data': robot.sensor_data,\n",
    "                    'crash': robot.crash,\n",
    "                    'time': robot.time\n",
    "                }\n",
    "                reward, done = reward_model.calculate_reward(robot_state)\n",
    "                \n",
    "                # Store transition\n",
    "                ppo_agent.store_transition(state, action, reward, next_state, done, log_prob)\n",
    "                \n",
    "                # Update state and reward\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step += 1\n",
    "                \n",
    "                # Visualize and record at end of episode\n",
    "                if done or step == max_steps-1:\n",
    "                    if episode in [0, ppo_episodes//2, ppo_episodes-1]:\n",
    "                        environment.map.blit(map, (0, 0))\n",
    "                        robot.draw(environment.map)\n",
    "                        environment.robot_frames([robot.x, robot.y], robot.theta)\n",
    "                        environment.robot_sensor([robot.x, robot.y], robot.points)\n",
    "                        \n",
    "                        # Add info text\n",
    "                        font = pygame.font.Font('freesansbold.ttf', 24)\n",
    "                        text = font.render(f\"PPO Individual {individual_idx+1}, Episode {episode+1}\", True, (255, 255, 255), (0, 0, 0))\n",
    "                        environment.map.blit(text, (10, 10))\n",
    "                        \n",
    "                        # Record frame\n",
    "                        frame = surface_to_array(environment.map)\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                        video_writer.write(frame)\n",
    "            \n",
    "            # Update policy\n",
    "            if (episode + 1) % 5 == 0:\n",
    "                ppo_agent.update()\n",
    "            \n",
    "            # Calculate distance to goal\n",
    "            distance = math.sqrt((robot.x - end_position[0])**2 + (robot.y - end_position[1])**2)\n",
    "            \n",
    "            # Store metrics\n",
    "            individual_rewards.append(episode_reward)\n",
    "            individual_distances.append(distance)\n",
    "            \n",
    "            print(f\"Individual {individual_idx+1}, Episode {episode+1}: Reward = {episode_reward:.2f}, Distance = {distance:.2f}\")\n",
    "        \n",
    "        # After training this individual, check if it's the best so far\n",
    "        final_reward = np.mean(individual_rewards[-5:])  # Average of last 5 episodes\n",
    "        if final_reward > best_reward:\n",
    "            best_reward = final_reward\n",
    "            best_model = ppo_agent.actor_critic\n",
    "            best_ppo_rewards = individual_rewards\n",
    "            best_ppo_distances = individual_distances\n",
    "            # Save this model\n",
    "            torch.save(ppo_agent.actor_critic.state_dict(), f\"hybrid_best_individual_{individual_idx+1}.pt\")\n",
    "    \n",
    "    # Save final best model\n",
    "    if best_model is not None:\n",
    "        torch.save(best_model.state_dict(), \"hybrid_best_model.pt\")\n",
    "    \n",
    "    # Save PPO metrics for the best individual\n",
    "    save_ppo_metrics(best_ppo_rewards, best_ppo_distances, 'hybrid_ppo_metrics.npz')\n",
    "    \n",
    "    # Close video writer\n",
    "    video_writer.release()\n",
    "    \n",
    "    print(\"\\nHybrid GA-PPO Simulation Complete!\")\n",
    "    return min_fitness_history, mean_fitness_history, best_ppo_rewards, best_ppo_distances, video_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hybrid GA-PPO Simulation\n",
      "Phase 1: Running Genetic Algorithm...\n",
      "GA Generation 1/10\n",
      "Generation 1 completed. Best distance: 723.89, Mean: 814.31\n",
      "GA Generation 2/10\n",
      "Generation 2 completed. Best distance: 724.01, Mean: 802.93\n",
      "GA Generation 3/10\n",
      "Generation 3 completed. Best distance: 723.15, Mean: 796.04\n",
      "GA Generation 4/10\n",
      "Generation 4 completed. Best distance: 723.52, Mean: 792.01\n",
      "GA Generation 5/10\n",
      "Generation 5 completed. Best distance: 723.34, Mean: 776.11\n",
      "GA Generation 6/10\n",
      "Generation 6 completed. Best distance: 723.38, Mean: 783.27\n",
      "GA Generation 7/10\n",
      "Generation 7 completed. Best distance: 723.00, Mean: 787.06\n",
      "GA Generation 8/10\n",
      "Generation 8 completed. Best distance: 722.38, Mean: 773.05\n",
      "GA Generation 9/10\n",
      "Generation 9 completed. Best distance: 722.99, Mean: 783.65\n",
      "GA Generation 10/10\n"
     ]
    }
   ],
   "source": [
    "# Run hybrid simulation with fewer generations/episodes for demonstration\n",
    "ga_fitness_min, ga_fitness_mean, ppo_rewards, ppo_distances, video_file = run_hybrid_ga_ppo(\n",
    "    ga_generations=10,  # Reduced for demonstration\n",
    "    ppo_episodes=100,    # Reduced for demonstration\n",
    "    population_size=200, # Reduced for demonstration\n",
    "    top_n_individuals=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# GA phase metrics\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(range(1, len(ga_fitness_min) + 1), ga_fitness_min, 'b-', label='Best Distance')\n",
    "plt.plot(range(1, len(ga_fitness_mean) + 1), ga_fitness_mean, 'r-', label='Mean Distance')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Distance to Goal (pixels)')\n",
    "plt.title('GA Phase: Distance Evolution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# PPO phase metrics\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(range(1, len(ppo_rewards) + 1), ppo_rewards, 'g-')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('PPO Phase: Reward Evolution')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(range(1, len(ppo_distances) + 1), ppo_distances, 'm-')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Distance to Goal (pixels)')\n",
    "plt.title('PPO Phase: Distance Evolution')\n",
    "plt.grid(True)\n",
    "\n",
    "# Combined plot - compare final GA distance with PPO distance\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(['GA Initial', 'GA Final', 'PPO Final'], \n",
    "       [ga_fitness_mean[0], ga_fitness_min[-1], ppo_distances[-1]], \n",
    "       color=['blue', 'green', 'purple'])\n",
    "plt.ylabel('Distance to Goal (pixels)')\n",
    "plt.title('Comparison: Initial vs Final Distances')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hybrid_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the video\n",
    "Video(video_file, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Approach Analysis\n",
    "\n",
    "### Benefits of the Hybrid Approach:\n",
    "\n",
    "1. **Exploration and Exploitation Balance**: GA provides broad exploration of the solution space, while PPO offers deep exploitation through gradient-based optimization.\n",
    "\n",
    "2. **Warm Starting**: Using GA to generate initial policies gives PPO a better starting point, potentially avoiding local optima.\n",
    "\n",
    "3. **Computational Efficiency**: Instead of running PPO from scratch on random policies, we focus computational resources on promising solutions.\n",
    "\n",
    "4. **Robustness**: The hybrid approach combines the creativity of evolutionary methods with the precision of reinforcement learning.\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "1. **Complexity**: The hybrid approach is more complex to implement and tune.\n",
    "\n",
    "2. **Computational Cost**: Running both GA and PPO requires more computation than either approach alone.\n",
    "\n",
    "3. **Architecture Conversion**: Converting between the neural network architectures in GA and PPO requires careful implementation.\n",
    "\n",
    "4. **Hyperparameter Sensitivity**: The approach introduces additional hyperparameters that need tuning (e.g., when to switch from GA to PPO, how many individuals to fine-tune)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-mmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
